{
  "metadata": {
    "name": "mnist-pytorch abhishek uniform sampler",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nsc\nspark \u003d SparkSession.builder.appName(\"Pytorch_test\").getOrCreate()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nimport os\nos.environ[\"SPARK_LOCAL_DIRS\"]"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nresults_dir \u003d \"/users/brenton/pyspark-ml/mnist-pytorch\"\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nimport os\nfrom pyspark.sql import SparkSession\nfrom app import main\n\nos.environ[\u0027PYSPARK_PYTHON\u0027] \u003d \"./environment/bin/python\"\nspark \u003d SparkSession.builder.config(\n    \"spark.archives\",  # \u0027spark.yarn.dist.archives\u0027 in YARN.\n    \"/mnt/pyenv/mnist_pytorch.tar.gz#environment\"\n    #).config(\"spark.pyspark.virtualenv.enabled\", True\n    #).config(\"spark.pyspark.virtualenv.type\", \"native\"\n    ).getOrCreate()\n#main(spark)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nspark.conf.get(f\"spark.pyspark.virtualenv.enabled\")\nspark.conf.get(f\"spark.pyspark.virtualenv.type\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nsc.install_packages(\"pyarrow\")\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\ndef init_logging(fname):\n    import logging\n    formatter \u003d logging.Formatter(\u0027%(asctime)s|%(msecs)04d|%(name)s|%(levelname)s|%(message)s\u0027, datefmt\u003d\u0027%H:%M:%S\u0027)\n    handler \u003d logging.FileHandler(fname, mode\u003d\u0027a\u0027)\n    handler.setFormatter(formatter)\n    logger \u003d logging.getLogger(\u0027Pytorch_test\u0027)\n    logger.setLevel(logging.INFO)\n    logger.addHandler(handler)\n    \n    return logger\n"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom pyspark.ml.torch.distributor import TorchDistributor\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 \u003d nn.Conv2d(1, 32, 3, 1)\n        self.conv2 \u003d nn.Conv2d(32, 64, 3, 1)\n        self.dropout1 \u003d nn.Dropout(0.25)\n        self.dropout2 \u003d nn.Dropout(0.5)\n        self.fc1 \u003d nn.Linear(9216, 128)\n        self.fc2 \u003d nn.Linear(128, 10)\n        \n\n    def forward(self, x):\n        x \u003d self.conv1(x)\n        x \u003d F.relu(x)\n        x \u003d self.conv2(x)\n        x \u003d F.relu(x)\n        x \u003d F.max_pool2d(x, 2)\n        x \u003d self.dropout1(x)\n        x \u003d torch.flatten(x, 1)\n        x \u003d self.fc1(x)\n        x \u003d F.relu(x)\n        x \u003d self.dropout2(x)\n        x \u003d self.fc2(x)\n        output \u003d F.log_softmax(x, dim\u003d1)\n        return output"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nfrom torchvision import datasets\nfrom typing import Union, Optional, Callable, Tuple, Any\nfrom pathlib import Path\nimport codecs\nimport numpy as np\nfrom PIL import Image\n\ndef get_int(b: bytes) -\u003e int:\n    return int(codecs.encode(b, \"hex\"), 16)\n\n\nSN3_PASCALVINCENT_TYPEMAP \u003d {\n    8: torch.uint8,\n    9: torch.int8,\n    11: torch.int16,\n    12: torch.int32,\n    13: torch.float32,\n    14: torch.float64,\n}\n\nclass LazyLoad_MNIST(datasets.VisionDataset):\n    def __init__(\n        self,\n        root: Union[str, Path],\n        train: bool \u003d True,\n        transform: Optional[Callable] \u003d None,\n        target_transform: Optional[Callable] \u003d None,\n        download: bool \u003d False,\n        rank: int \u003d -1\n    ) -\u003e None:\n        super().__init__(root, transform\u003dtransform, target_transform\u003dtarget_transform)\n        self.folder_path \u003d root\n        self.rank \u003d int(rank)+1\n        self.data, self.targets \u003d self._load_data()\n\n    def _load_data(self):\n        image_file \u003d str(self.rank)+\"-mnist-patterns-idx3-ubyte\"\n        data \u003d self.read_image_file(os.path.join(self.folder_path,image_file))\n\n        label_file \u003d str(self.rank)+\"-mnist-labels-idx1-ubyte\"\n        targets \u003d self.read_label_file(os.path.join(self.folder_path,label_file))\n\n        return data, targets\n    \n    def read_sn3_pascalvincent_tensor(self, path: str, strict: bool \u003d True) -\u003e torch.Tensor:\n        \"\"\"Read a SN3 file in \"Pascal Vincent\" format (Lush file \u0027libidx/idx-io.lsh\u0027).\n        Argument may be a filename, compressed filename, or file object.\n        \"\"\"\n        # read\n        with open(path, \"rb\") as f:\n            data \u003d f.read()\n    \n        # parse\n        if sys.byteorder \u003d\u003d \"little\":\n            magic \u003d get_int(data[0:4])\n            nd \u003d magic % 256\n            ty \u003d magic // 256\n        else:\n            nd \u003d get_int(data[0:1])\n            ty \u003d get_int(data[1:2]) + get_int(data[2:3]) * 256 + get_int(data[3:4]) * 256 * 256\n    \n        assert 1 \u003c\u003d nd \u003c\u003d 3\n        assert 8 \u003c\u003d ty \u003c\u003d 14\n        torch_type \u003d SN3_PASCALVINCENT_TYPEMAP[ty]\n        s \u003d [get_int(data[4 * (i + 1) : 4 * (i + 2)]) for i in range(nd)]\n    \n        if sys.byteorder \u003d\u003d \"big\":\n            for i in range(len(s)):\n                s[i] \u003d int.from_bytes(s[i].to_bytes(4, byteorder\u003d\"little\"), byteorder\u003d\"big\", signed\u003dFalse)\n    \n        parsed \u003d torch.frombuffer(bytearray(data), dtype\u003dtorch_type, offset\u003d(4 * (nd + 1)))\n    \n        # The MNIST format uses the big endian byte order, while `torch.frombuffer` uses whatever the system uses. In case\n        # that is little endian and the dtype has more than one byte, we need to flip them.\n        if sys.byteorder \u003d\u003d \"little\" and parsed.element_size() \u003e 1:\n            parsed \u003d _flip_byte_order(parsed)\n    \n        assert parsed.shape[0] \u003d\u003d np.prod(s) or not strict\n        return parsed.view(*s)\n\n\n    def read_label_file(self, path: str) -\u003e torch.Tensor:\n        x \u003d self.read_sn3_pascalvincent_tensor(path, strict\u003dFalse)\n        if x.dtype !\u003d torch.uint8:\n            raise TypeError(f\"x should be of dtype torch.uint8 instead of {x.dtype}\")\n        if x.ndimension() !\u003d 1:\n            raise ValueError(f\"x should have 1 dimension instead of {x.ndimension()}\")\n        return x.long()\n\n\n    def read_image_file(self, path: str) -\u003e torch.Tensor:\n        x \u003d self.read_sn3_pascalvincent_tensor(path, strict\u003dFalse)\n        if x.dtype !\u003d torch.uint8:\n            raise TypeError(f\"x should be of dtype torch.uint8 instead of {x.dtype}\")\n        if x.ndimension() !\u003d 3:\n            raise ValueError(f\"x should have 3 dimension instead of {x.ndimension()}\")\n        return x\n    \n    def __getitem__(self, index: int) -\u003e Tuple[Any, Any]:\n        \"\"\"\n        Args:\n            index (int): Index\n\n        Returns:\n            tuple: (image, target) where target is index of the target class.\n        \"\"\"\n        img, target \u003d self.data[index], int(self.targets[index])\n\n        # doing this so that it is consistent with all other datasets\n        # to return a PIL Image\n        img \u003d Image.fromarray(img.numpy(), mode\u003d\"L\")\n\n        if self.transform is not None:\n            img \u003d self.transform(img)\n\n        if self.target_transform is not None:\n            target \u003d self.target_transform(target)\n\n        return img, target\n\n\n    def __len__(self) -\u003e int:\n        return len(self.data)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\ntransform\u003dtransforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n        ])\nLazyLoad_MNIST(\u0027/mnt/infimnist/mnist-split\u0027, train\u003dTrue, download\u003dFalse, transform\u003dtransform, rank\u003d\"0\")"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nimport os\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.optim.lr_scheduler import StepLR\nimport torch.distributed as dist\n\ndef getTrainDataset(rank):\n    transform\u003dtransforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n        ])\n    \n    train8m_kwargs \u003d {\u0027batch_size\u0027: 3125}\n    trainset8m \u003d LazyLoad_MNIST(\u0027/mnt/mnist-split/mnist-split\u0027, train\u003dTrue, download\u003dFalse, transform\u003dtransform, rank\u003drank)\n    \n    return trainset8m, train8m_kwargs\n\ndef getTestLoader():\n    transform\u003dtransforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n        ])\n    \n    test8m_kwargs \u003d {\u0027batch_size\u0027: 1000}\n    testset8m \u003d datasets.MNIST(\u0027/mnt/MNIST/\u0027, train\u003dFalse, download\u003dFalse, transform\u003dtransform)\n    test_loader \u003d torch.utils.data.DataLoader(testset8m, **test8m_kwargs)\n    \n    return test_loader\n"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\ndef train(model, device, getTrainDataset, optimizer, epoch, scheduler):\n    import os\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    import torch.optim as optim\n    from torchvision import datasets, transforms\n    from torch.optim.lr_scheduler import StepLR\n    import torch.distributed as dist\n    from torch.nn.parallel import DistributedDataParallel as DDP\n    from torch.utils.data.distributed import DistributedSampler\n    import logging\n    import time\n    import math\n    import warnings\n    warnings.filterwarnings(\"ignore\")\n\n    \n    mu, sigma \u003d 1562.5, 400 # mean and standard deviation\n    sample_size \u003d np.random.normal(mu, sigma, 3125)\n    sample_size \u003d [max(0,min(math.ceil(i),3125)) for i in sample_size]\n    \n    \n    print(\"Running distributed training\")\n    dist.init_process_group(\"gloo\")\n    fname \u003d results_dir+\u0027/data_split_logs_1562_400_25/log_rank_{}.log\u0027.format(os.environ[\u0027RANK\u0027])\n    logger \u003d init_logging(fname)\n    \n    ddp_model \u003d DDP(model)\n    ddp_model.train()\n    \n    def _backward_hook(module, grad_input, grad_output):\n        logger.info(\"{}|Loss Grad End|Rank {}|Epoch {}|Iteration {}\".format(time.time(),os.environ[\u0027RANK\u0027],epoch,batch_idx))\n    hook_handler \u003d ddp_model.module.conv1.register_full_backward_hook(_backward_hook)\n    \n    # The sampler returns a iterator over indices, which are fed into dataloader to bachify\n    train_dataset, train8m_kwargs \u003d getTrainDataset(os.environ[\u0027RANK\u0027])\n    # training_sampler \u003d DistributedSampler(train_dataset, shuffle\u003dTrue)\n    # train_loader \u003d torch.utils.data.DataLoader(train_dataset, sampler\u003dtraining_sampler, **train8m_kwargs)\n    train_loader \u003d torch.utils.data.DataLoader(train_dataset, **train8m_kwargs)\n    \n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target \u003d data[:sample_size[batch_idx]], target[:sample_size[batch_idx]]\n        #time.sleep(int(os.environ[\u0027RANK\u0027]))\n        #logger.info(\"{}|Start|Rank {}|Epoch {}|Iteration {}\".format(time.time(),os.environ[\u0027RANK\u0027],epoch,batch_idx))\n        #dist.barrier()\n        data, target \u003d data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output \u003d ddp_model(data)\n        loss \u003d F.nll_loss(output, target)\n        logger.info(\"{}|Loss Start|Rank {}|Epoch {}|Iteration {}\".format(time.time(),os.environ[\u0027RANK\u0027],epoch,batch_idx))\n        loss.backward()\n        logger.info(\"{}|Sync End|Rank {}|Epoch {}|Iteration {}\".format(time.time(),os.environ[\u0027RANK\u0027],epoch,batch_idx))\n        optimizer.step()\n\n        if batch_idx % 10 \u003d\u003d 0:\n            print(\u0027[Rank{}]Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\u0027.format(os.environ[\u0027RANK\u0027],\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n    \n    \n    non_ddp_model \u003d ddp_model.module\n    hook_handler.remove()\n    handlers \u003d logger.handlers[:]\n    for handler in handlers:\n        logger.removeHandler(handler)\n        handler.close()\n\n    dist.destroy_process_group() #DDP anything cannot exist after this point\n    return (non_ddp_model, optimizer, scheduler)\n\n\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss \u003d 0\n    correct \u003d 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target \u003d data.to(device), target.to(device)\n            output \u003d model(data)\n            test_loss +\u003d F.nll_loss(output, target, reduction\u003d\u0027sum\u0027).item()  # sum up batch loss\n            pred \u003d output.argmax(dim\u003d1, keepdim\u003dTrue)  # get the index of the max log-probability\n            correct +\u003d pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /\u003d len(test_loader.dataset)\n\n    print(\u0027\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\u0027.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\ndef dst_train(getTrainDataset, getTestLoader, num_proc):\n\n    torch.manual_seed(1)\n    device \u003d torch.device(\"cpu\")\n    model \u003d Net().to(device)\n    \n    optimizer \u003d optim.Adadelta(model.parameters(), lr\u003d1)\n    scheduler \u003d StepLR(optimizer, step_size\u003d1, gamma\u003d0.7)\n    test_ldr \u003d getTestLoader()\n\n    for epoch in range(1, 25 + 1):\n        model, optimizer, scheduler \u003d TorchDistributor(num_processes\u003dnum_proc, local_mode\u003dFalse, use_gpu\u003dFalse).run(train, model, device, getTrainDataset, optimizer, epoch, scheduler)\n        test(model, device, test_ldr)\n        scheduler.step()\n    \n    return \"Finished\"\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Packing a virtual environment to distribute to the cluster\n\nMany options explained here:\nhttps://spark.apache.org/docs/latest/api/python/user_guide/python_packaging.html\n\nI\u0027m using virtual enviromnents, because I\u0027m familiar with that, and am not using conda or anything ATM.\n\npython -m venv mnist_pytorch\nsource mnist_pytorch/bin/activate\npip install pyarrow pandas venv-pack\n"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\ndst_train(dataset1,test_loader,4)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Potential ways to make it faster - \n1. Have the data on the local (/mnt/) instead of NAS(/users/brenton/pyspark-ml/mnist-pytorch)\n2. Each worker has 16 cores but only 1 used per machine\n"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\ndst_train(getTrainDataset, getTestLoader, 16)"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\ndst_train(dataset1,test_loader,3)"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\ndst_train(dataset1,test_loader,2)"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\ndst_train(dataset1,test_loader,1)"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\noutput_dist \u003d TorchDistributor(num_processes\u003d4, local_mode\u003dFalse, use_gpu\u003dFalse).run(main)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\ndef dst_train_and_test(train_dataset, test_ldr):\n\n    torch.manual_seed(1)\n    device \u003d torch.device(\"cpu\")\n    model \u003d Net().to(device)\n    \n    optimizer \u003d optim.Adadelta(model.parameters(), lr\u003d1)\n    scheduler \u003d StepLR(optimizer, step_size\u003d1, gamma\u003d0.7)\n\n    for epoch in range(1, 4 + 1):\n        model, optimizer, scheduler \u003d loadModelFromCheckpoint() if epoch \u003e 1 else (model, optimizer, scheduler)\n        TorchDistributor(num_processes\u003d4, local_mode\u003dFalse, use_gpu\u003dFalse).run(train_and_test, model, device, train_dataset, optimizer, epoch,test_ldr, scheduler)\n    return \"Finished\""
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\ndst_train_and_test(dataset1, test_loader)"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx\u003d[1,2,3,4]\ny\u003d[40.33,24.33,18.36,15.88]\nplt.scatter(x,y)\n\nu \u003d np.linspace(0.5, 9, 400)\nv \u003d np.linspace(0.5, 60, 400)\nu, v \u003d np.meshgrid(u, v)\nplt.contour(u, v, (u*(v-5)), [40], colors\u003d\u0027k\u0027) #There seems to be a 5sec constant added to the total times, presumably because of test step (which takes place in one machine only)\n\nplt.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Task Wait Times -  \nSync_End[i+1] - Loss_End[i]"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Data Multi Split Wait Times"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nlogs_arr \u003d []\nfor i in range(16):\n    logfile \u003d  open(results_dir+\"/data_split_logs/log_rank_{}.log\".format(i), \"r\")\n    lines \u003d logfile.readlines()\n    logs \u003d []\n    for line in lines:\n        logs.append(line.split(\u0027|\u0027))\n    logfile.close()\n    logs_arr.append(list(reversed(logs)))\n\ndatapoints \u003d []\nfor logs in logs_arr:\n    i \u003d 0\n    while i\u003clen(logs):\n        if logs[i][5]\u003d\u003d\"Sync End\":\n            datapoints.append(float(logs[i][4])-float(logs[i+1][4]))\n        i+\u003d1\nlen(datapoints)\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# plt.style.use(\u0027_mpl-gallery\u0027)\n\n# make data:\ny \u003d datapoints\nn \u003d len(y)\nx \u003d 0.5+np.arange(n)\n\nplt.figure(figsize\u003d(20,5))\nplt.plot(x,y, \u0027-o\u0027)\nplt.show()\n\nplt.figure(figsize\u003d(20, 5))\nplt.hist(y, bins\u003d50)\nplt.show()\n\nplt.figure(figsize\u003d(20, 5))\nplt.hist(y, bins\u003d50, log\u003dTrue)\nplt.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Norm Dist of Data - Wait times"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nlogs_arr \u003d []\nfor i in range(16):\n    logfile \u003d  open(results_dir+\"/norm_dist_logs/log_rank_{}.log\".format(i), \"r\")\n    lines \u003d logfile.readlines()\n    logs \u003d []\n    for line in lines:\n        logs.append(line.split(\u0027|\u0027))\n    logfile.close()\n    logs_arr.append(list(reversed(logs)))\n\ndatapoints \u003d []\nprint(logs_arr[0][0])\n\ndef is_same_epoch(log1, log2):\n    log1_epoch \u003d int((log1[7].split(\" \"))[1])\n    log2_epoch \u003d int((log2[7].split(\" \"))[1])\n    return log1_epoch\u003d\u003dlog2_epoch\n\nfor logs in logs_arr:\n    i \u003d 0\n    while i\u003clen(logs):\n        if logs[i][5]\u003d\u003d\"Sync End\" and is_same_epoch(logs[i], logs[i+1]):\n            datapoints.append(float(logs[i][4])-float(logs[i+1][4]))\n        i+\u003d1\nlen(datapoints)\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# plt.style.use(\u0027_mpl-gallery\u0027)\n\n# make data:\ny \u003d datapoints\nn \u003d len(y)\nx \u003d 0.5+np.arange(n)\n\nplt.figure(figsize\u003d(20,5))\nplt.plot(x,y, \u0027-o\u0027)\nplt.xlabel(\"data points\")\nplt.ylabel(\"Time in seconds\")\nplt.show()\n\n\nplt.figure(figsize\u003d(20, 5))\nplt.hist(y, bins\u003d50)\nplt.show()\n\nplt.figure(figsize\u003d(20, 5))\nplt.hist(y, bins\u003d50, log\u003dTrue)\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Norm Dist of data (Mu-512, SD-100, Epoch-25)- Wait times"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nlogs_arr \u003d []\nfor i in range(16):\n    logfile \u003d  open(results_dir+\"/data_split_logs_512_100_25/log_rank_{}.log\".format(i), \"r\")\n    lines \u003d logfile.readlines()\n    logs \u003d []\n    for line in lines:\n        logs.append(line.split(\u0027|\u0027))\n    logfile.close()\n    logs_arr.append(list(reversed(logs)))\n\ndatapoints \u003d []\nprint(logs_arr[0][0])\n\ndef is_same_epoch(log1, log2):\n    log1_epoch \u003d int((log1[7].split(\" \"))[1])\n    log2_epoch \u003d int((log2[7].split(\" \"))[1])\n    return log1_epoch\u003d\u003dlog2_epoch\n\nfor logs in logs_arr:\n    i \u003d 0\n    while i\u003clen(logs):\n        if logs[i][5]\u003d\u003d\"Sync End\" and is_same_epoch(logs[i], logs[i+1]):\n            datapoints.append(float(logs[i][4])-float(logs[i+1][4]))\n        i+\u003d1\nlen(datapoints)\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# plt.style.use(\u0027_mpl-gallery\u0027)\n\n# make data:\ny \u003d datapoints\nn \u003d len(y)\nx \u003d 0.5+np.arange(n)\n\nplt.figure(figsize\u003d(20,5))\nplt.plot(x,y, \u0027-o\u0027)\nplt.xlabel(\"data points\")\nplt.ylabel(\"Time in seconds\")\nplt.show()\n\n\nplt.figure(figsize\u003d(20, 5))\nplt.hist(y, bins\u003d250)\nplt.show()\n\nplt.figure(figsize\u003d(20, 5))\nplt.hist(y, bins\u003d50, log\u003dTrue)\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Norm Dist of data (Mu-1562, SD-400, Epoch-25)- Wait times"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nlogs_arr \u003d []\nfor i in range(16):\n    logfile \u003d  open(results_dir+\"/data_split_logs_1562_400_25/log_rank_{}.log\".format(i), \"r\")\n    lines \u003d logfile.readlines()\n    logs \u003d []\n    for line in lines:\n        logs.append(line.split(\u0027|\u0027))\n    logfile.close()\n    logs_arr.append(list(reversed(logs)))\n\ndatapoints \u003d []\nprint(logs_arr[0][0])\n\ndef is_same_epoch(log1, log2):\n    log1_epoch \u003d int((log1[7].split(\" \"))[1])\n    log2_epoch \u003d int((log2[7].split(\" \"))[1])\n    return log1_epoch\u003d\u003dlog2_epoch\n\nfor logs in logs_arr:\n    i \u003d 0\n    while i\u003clen(logs):\n        if logs[i][5]\u003d\u003d\"Sync End\" and is_same_epoch(logs[i], logs[i+1]):\n            datapoints.append(float(logs[i][4])-float(logs[i+1][4]))\n        i+\u003d1\nlen(datapoints)\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# plt.style.use(\u0027_mpl-gallery\u0027)\n\n# make data:\ny \u003d datapoints\nn \u003d len(y)\nx \u003d 0.5+np.arange(n)\n\nplt.figure(figsize\u003d(20,5))\nplt.plot(x,y, \u0027-o\u0027)\nplt.xlabel(\"data points\")\nplt.ylabel(\"Time in seconds\")\nplt.show()\n\n\nplt.figure(figsize\u003d(20, 5))\nplt.hist(y, bins\u003d500)\nplt.show()\n\nplt.figure(figsize\u003d(20, 5))\nplt.hist(y, bins\u003d50, log\u003dTrue)\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Task Service Times -  \nLoss_Grad_End[i+1] - Sync_End[i]"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Norm Dist of data - Service times"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nlogs_arr \u003d []\nfor i in range(16):\n    logfile \u003d  open(results_dir+\"/norm_dist_logs/log_rank_{}.log\".format(i), \"r\")\n    lines \u003d logfile.readlines()\n    logs \u003d []\n    for line in lines:\n        logs.append(line.split(\u0027|\u0027))\n    logfile.close()\n    logs_arr.append(list(reversed(logs)))\n\ndatapoints \u003d []\n\ndef is_same_epoch(log1, log2):\n    log1_epoch \u003d int((log1[7].split(\" \"))[1])\n    log2_epoch \u003d int((log2[7].split(\" \"))[1])\n    return log1_epoch\u003d\u003dlog2_epoch\n\nfor logs in logs_arr:\n    i \u003d 0\n    while i\u003clen(logs)-2:\n        if logs[i][5]\u003d\u003d\"Loss Grad End\" and is_same_epoch(logs[i], logs[i+2]):\n            datapoints.append(float(logs[i][4])-float(logs[i+2][4]))\n        i+\u003d1\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# plt.style.use(\u0027_mpl-gallery\u0027)\n\n# make data:\ny \u003d [i for i in datapoints if i\u003c100] # Removing some outliers\nn \u003d len(y)\nx \u003d 0.5+np.arange(n)\n\nplt.figure(figsize\u003d(20,5))\nplt.plot(x,y, \u0027-o\u0027)\nplt.xlabel(\"data points\")\nplt.ylabel(\"Time in seconds\")\nplt.show()\n\nplt.figure(figsize\u003d(20, 5))\nplt.hist(y, bins\u003d50)\nplt.show()\n\nplt.figure(figsize\u003d(20, 5))\nplt.hist(y, bins\u003d50, log\u003dTrue)\nplt.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Norm Dist of data (Mu-512, SD-100, Epoch-25)- Service times"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nlogs_arr \u003d []\nfor i in range(16):\n    logfile \u003d  open(results_dir+\"/data_split_logs_512_100_25/log_rank_{}.log\".format(i), \"r\")\n    lines \u003d logfile.readlines()\n    logs \u003d []\n    for line in lines:\n        logs.append(line.split(\u0027|\u0027))\n    logfile.close()\n    logs_arr.append(list(reversed(logs)))\n\ndatapoints \u003d []\n\ndef is_same_epoch(log1, log2):\n    log1_epoch \u003d int((log1[7].split(\" \"))[1])\n    log2_epoch \u003d int((log2[7].split(\" \"))[1])\n    return log1_epoch\u003d\u003dlog2_epoch\n\nfor logs in logs_arr:\n    i \u003d 0\n    while i\u003clen(logs)-2:\n        if logs[i][5]\u003d\u003d\"Loss Grad End\" and is_same_epoch(logs[i], logs[i+2]):\n            datapoints.append(float(logs[i][4])-float(logs[i+2][4]))\n        i+\u003d1\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# plt.style.use(\u0027_mpl-gallery\u0027)\n\n# make data:\n# y \u003d [i for i in datapoints if i\u003c100] # Removing some outliers\ny \u003d datapoints\nn \u003d len(y)\nx \u003d 0.5+np.arange(n)\n\nplt.figure(figsize\u003d(20,5))\nplt.plot(x,y, \u0027-o\u0027)\nplt.xlabel(\"data points\")\nplt.ylabel(\"Time in seconds\")\nplt.show()\n\nplt.figure(figsize\u003d(20, 5))\nplt.hist(y, bins\u003d500)\nplt.show()\n\nplt.figure(figsize\u003d(20, 5))\nplt.hist(y, bins\u003d50, log\u003dTrue)\nplt.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Norm Dist of data (Mu-1562.5, SD-400, Epoch-25)- Service times"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nlogs_arr \u003d []\nfor i in range(16):\n    logfile \u003d  open(results_dir+\"/data_split_logs_1562_400_25/log_rank_{}.log\".format(i), \"r\")\n    lines \u003d logfile.readlines()\n    logs \u003d []\n    for line in lines:\n        logs.append(line.split(\u0027|\u0027))\n    logfile.close()\n    logs_arr.append(list(reversed(logs)))\n\ndatapoints \u003d []\n\ndef is_same_epoch(log1, log2):\n    log1_epoch \u003d int((log1[7].split(\" \"))[1])\n    log2_epoch \u003d int((log2[7].split(\" \"))[1])\n    return log1_epoch\u003d\u003dlog2_epoch\n\nfor logs in logs_arr:\n    i \u003d 0\n    while i\u003clen(logs)-2:\n        if logs[i][5]\u003d\u003d\"Loss Grad End\" and is_same_epoch(logs[i], logs[i+2]):\n            datapoints.append(float(logs[i][4])-float(logs[i+2][4]))\n        i+\u003d1\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# plt.style.use(\u0027_mpl-gallery\u0027)\n\n# make data:\n# y \u003d [i for i in datapoints if i\u003c100] # Removing some outliers\ny \u003d datapoints\nn \u003d len(y)\nx \u003d 0.5+np.arange(n)\n\nplt.figure(figsize\u003d(20,5))\nplt.plot(x,y, \u0027-o\u0027)\nplt.xlabel(\"data points\")\nplt.ylabel(\"Time in seconds\")\nplt.show()\n\nplt.figure(figsize\u003d(20, 5))\nplt.hist(y, bins\u003d500)\nplt.show()\n\nplt.figure(figsize\u003d(20, 5))\nplt.hist(y, bins\u003d50, log\u003dTrue)\nplt.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n#Infimnist Data Gen Script\nfor i in range(1,33):\n    start \u003d str(i*100000+10000)\n    end \u003d str((i+1)*100000+10000-1)\n    print(\"./infimnist lab \"+start+\" \"+end+\" \u003e mnist-split/\"+str(i)+\"-mnist-labels-idx1-ubyte\")\n    print(\"./infimnist pat \"+start+\" \"+end+\" \u003e mnist-split/\"+str(i)+\"-mnist-patterns-idx3-ubyte\")"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# Testing different Distributions\nimport math\nmu, sigma \u003d 1562.5, 400 # mean and standard deviation\ns \u003d np.random.normal(mu, sigma, 3125)\ns \u003d [max(0,min(math.ceil(i),3125)) for i in s]\nplt.figure(figsize\u003d(20,5))\nplt.hist(s, bins\u003d200)\nplt.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n"
    }
  ]
}